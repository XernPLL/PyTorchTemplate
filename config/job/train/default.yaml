# @package _global_
name: 'First_training'
working_dir: "."
device: 'cuda'
random_seed: ~
num_epoch: 1000  # max iteration
data:
  train_dir: 'dataset/meta/train'
  test_dir: 'dataset/meta/test'
  file_format: '*.file_extension'
  use_background_generator: true
  divide_dataset_per_gpu: true
train:
  num_workers: 4
  batch_size: 64
  optimizer:
    mode: 'adam'
    adam:
      lr: 0.001
      betas:
        - 0.9
        - 0.999
test:
  num_workers: 4
  batch_size: 64
model:
  ~
dist:
  master_addr: 'localhost'
  master_port: '12355'
  mode: 'nccl'
  gpus: 0 # 0 for not using dist, -1 for using all gpus
  timeout: 30 # seconds for timeout. In nccl mode, set ~ for not using timeout
log:
  use_tensorboard: false
  use_wandb: true
  wandb_init_conf:
    name: ${name}
    entity: "xernpl"
    project: "Template"
  summary_interval: 10 # interval of step
  chkpt_interval: 10 # interval of epoch
  chkpt_dir: 'chkpt'
load:
  wandb_load_path: ~
  network_chkpt_path: ~
  strict_load: false
  resume_state_path: ~

#load:
#  wandb_load_path: "xernpl/uncategorized/709xg3af"
#  network_chkpt_path: ~
#  strict_load: false
#  resume_state_path: "${log.chkpt_dir}/First_training_94637.state"